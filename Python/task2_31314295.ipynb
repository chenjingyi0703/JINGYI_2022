{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t14-LmgSfl6v"
   },
   "source": [
    "# Assessment 1 Parsing Data And Text Preprocessing\n",
    "## Task 2: Text Pre-Processing\n",
    "*   Name: Jingyi Chen\n",
    "*   ID: 31314295"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJVoU7qKijYr"
   },
   "source": [
    "### 1.0 Initializing the Environment\n",
    "> Load the google drive path to read the data source;\n",
    "\n",
    "> Import the packages needed for programming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12318,
     "status": "ok",
     "timestamp": 1649381422173,
     "user": {
      "displayName": "Jingyi Chen",
      "userId": "01575192393336659630"
     },
     "user_tz": -600
    },
    "id": "_FAJz-SE9wC7",
    "outputId": "301a4c50-1249-4d4a-e02d-eb0b6f8e7751"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "# This will prompt for authorization.\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3166,
     "status": "ok",
     "timestamp": 1649381425335,
     "user": {
      "displayName": "Jingyi Chen",
      "userId": "01575192393336659630"
     },
     "user_tz": -600
    },
    "id": "k7aEmPg2cR0m",
    "outputId": "242f5b1e-1e0e-4a0a-908f-2d9a588af25b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the packages needed for programming. \n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BI0RjZ5rjo5i"
   },
   "source": [
    "### 2.0 Data Cleaning\n",
    "This part is mainly to clean the data as a whole. Because there are many pages of excel data, and the data on each page may be different. We need to clean the NA value and outliers of the data, and then merge the data together to facilitate our subsequent data operations and calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3515,
     "status": "ok",
     "timestamp": 1649381428846,
     "user": {
      "displayName": "Jingyi Chen",
      "userId": "01575192393336659630"
     },
     "user_tz": -600
    },
    "id": "kE55MWtaInHZ",
    "outputId": "796ef19c-2bf7-4106-9074-aaea69f0afb2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.io.excel._base.ExcelFile at 0x7f9a94349e10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the excel data from google drive\n",
    "directory = \"/content/drive/Shareddrives/FIT5196-s1-2022/A1/Task2/input_data/31314295.xlsx\"\n",
    "excel_data = pd.ExcelFile(directory)\n",
    "excel_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlSIlPtsnykv"
   },
   "source": [
    "There are 20 pages data in this excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1649381428847,
     "user": {
      "displayName": "Jingyi Chen",
      "userId": "01575192393336659630"
     },
     "user_tz": -600
    },
    "id": "2aH8ICUjFV2s",
    "outputId": "f2d8ff75-ce92-467c-ce8b-a0cb9247a934"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " '10',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the excel page number\n",
    "excel_data.sheet_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQXQ7exFlY1T"
   },
   "source": [
    "#### 2.1 Cleaning NA Data and Column Name Data \n",
    "This function **data_clean** is to clean a sheet of Excel data (according to week1- Week3 lessons). First load excel data, then delete all NA columns and all NA rows. As the operation of deleting NA causes the index of row and column number to be discontinuous, it is necessary to correct the number of rows and columns, and finally change the column names to 'reviewText', 'summary', 'reviewTime'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deQaBqdgaxql"
   },
   "outputs": [],
   "source": [
    "def data_clean(sheet_no):\n",
    "  df = excel_data.parse(sheet_no)\n",
    "  df = df.dropna(axis= 1, how = 'all')\n",
    "  df = df.dropna(axis= 0, how = 'all')\n",
    "  df.index = range(len(df.index))\n",
    "  df.columns = list(range(len(df.columns))) \n",
    "  df.columns = ['reviewText', 'summary', 'reviewTime']\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvQc4y_om8E3"
   },
   "source": [
    "First, excel data with sheet number 0 is cleaned, and then data on each page is cleaned through the **for loop**. Also in the loop, two piece of data in  Dataframe formats are **merged** according to the same column name through **concat** function. Because the **column names** in each sheet are read as data, some sheets produce a row of extra data. Data that removes column names from row data after concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aW68JWdOlhJ6"
   },
   "outputs": [],
   "source": [
    "df = data_clean('0')\n",
    "\n",
    "for sheet in excel_data.sheet_names:\n",
    "  if(sheet != '0'):\n",
    "      df_sheet = data_clean(sheet)\n",
    "      df = pd.concat([df_sheet, df], axis = 0, ignore_index=True)\n",
    "      df = df[~((df['reviewText'] == 'reviewText') & (df['summary'] == 'summary') & (df['reviewTime'] == 'reviewTime'))]\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCZ5NhJ3o7DF"
   },
   "source": [
    "#### 2.2 Data validation after cleaning \n",
    "After data cleaning, we ended up with **10000 rows** and **3 columns** in Dataframe format. And the **column name** of the data is correct. Check the first five lines to **make sure the data is properly cleaned**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1649381429512,
     "user": {
      "displayName": "Jingyi Chen",
      "userId": "01575192393336659630"
     },
     "user_tz": -600
    },
    "id": "S-fic8V75phL",
    "outputId": "4f160889-62ed-426c-bbc8-db3618bad349"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the row number and col number\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1649381429513,
     "user": {
      "displayName": "Jingyi Chen",
      "userId": "01575192393336659630"
     },
     "user_tz": -600
    },
    "id": "YSBKQHEY5ppM",
    "outputId": "bc330f90-e7d0-4768-93b3-4ee41b383f0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['reviewText', 'summary', 'reviewTime'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the cols name\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1649381429514,
     "user": {
      "displayName": "Jingyi Chen",
      "userId": "01575192393336659630"
     },
     "user_tz": -600
    },
    "id": "Dg1jMIQR9v1L",
    "outputId": "c8b6fab2-d587-405b-ac23-16af2af6c236"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-bf55973a-62e2-46c7-8c2c-91360aead363\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The snobs that think Ivan Reitman's film isn't...</td>\n",
       "      <td>Lighten Up, Folks...</td>\n",
       "      <td>12 6, 2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I was really surprised by this book on how it ...</td>\n",
       "      <td>5 Stars are not enough it should be 10+.</td>\n",
       "      <td>07 7, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The exact replacement for a APC RBC109 Battery...</td>\n",
       "      <td>This is not a exact replacement for a APC Batt...</td>\n",
       "      <td>01 18, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I bought this to clean the things on my desk i...</td>\n",
       "      <td>It really works</td>\n",
       "      <td>09 21, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I highly recommend \"V\" to anyone, who enjoys s...</td>\n",
       "      <td>Entertaining, suspenseful, and exciting!</td>\n",
       "      <td>01 20, 2006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bf55973a-62e2-46c7-8c2c-91360aead363')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-bf55973a-62e2-46c7-8c2c-91360aead363 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-bf55973a-62e2-46c7-8c2c-91360aead363');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                          reviewText  \\\n",
       "1  The snobs that think Ivan Reitman's film isn't...   \n",
       "2  I was really surprised by this book on how it ...   \n",
       "3  The exact replacement for a APC RBC109 Battery...   \n",
       "4  I bought this to clean the things on my desk i...   \n",
       "5  I highly recommend \"V\" to anyone, who enjoys s...   \n",
       "\n",
       "                                             summary   reviewTime  \n",
       "1                               Lighten Up, Folks...   12 6, 2010  \n",
       "2           5 Stars are not enough it should be 10+.   07 7, 2014  \n",
       "3  This is not a exact replacement for a APC Batt...  01 18, 2013  \n",
       "4                                    It really works  09 21, 2012  \n",
       "5           Entertaining, suspenseful, and exciting!  01 20, 2006  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the data of the first 5 rows \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sv4zLpwcpi3H"
   },
   "source": [
    "### 3.0 Text Preprocessing\n",
    " In this part, we mainly divide the sentences, and then do some cleaning work on the words, including removing the stop words, Rare tokens (with the threshold set to less than 10 days (i.e. 10 unique dates)), the words of the threshold to more than ceil(Number_of_days / 2). And modifying the words by stemming operation. Then finally completing the word frequency statistics and writing the data into the file.\n",
    "\n",
    " The following **seven steps** describe the overall word processing process in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDYKdGG6ltaF"
   },
   "source": [
    "####  First Step: \n",
    "We need to **convert the data format** to str, because the text data may contain some numeric data(Int, float), affecting the subsequent segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J7Ab9tZjvccv"
   },
   "outputs": [],
   "source": [
    "df = df.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VoILlYvRRe4I"
   },
   "source": [
    "Group **‘reviewTime’** fields using the built-in dataframe method group, find **‘reviewText’** of the same date, and then use **‘reviewTime’** as the key and **‘reviewText’** as the value of the array type. Then we get a dictionary type data, which can be caculated the number of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3b5iKN-A6PZR"
   },
   "outputs": [],
   "source": [
    "# First Step: Group **‘reviewTime’** fields, find **‘reviewText’** of the same date\n",
    "dict_word = dict()\n",
    "groups_= df.groupby('reviewTime').groups\n",
    "for key in groups_:\n",
    "  if key not in dict_word:\n",
    "    dict_word[key] = []\n",
    "  for index in groups_[key]:\n",
    "    dict_word[key].append(df['reviewText'][index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDU4YDKLRm9k"
   },
   "source": [
    "####  Second Step: \n",
    "\n",
    "According the regular expression **'[a-zA-Z]+(?:[-'][a-zA-Z]+)?'** to split the sentence to words by **re model**. Then lower case words by **lower function** and eliminate duplicate words by **set** type transform to prevent multiple words in a day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "moTtTOUcOizM"
   },
   "outputs": [],
   "source": [
    "# Second Step: split the sentence to words and eliminate duplicate words\n",
    "reg_tokens = re.compile(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "uniq_tokens = []\n",
    "\n",
    "# split the sentence to words\n",
    "for key in dict_word:\n",
    "    word_temps = []\n",
    "    words_string = \" \".join(dict_word[key])\n",
    "    temp = re.findall(reg_tokens, words_string)\n",
    "\n",
    "    # lower words\n",
    "    for word in temp:\n",
    "        word_temps.append(word.lower())\n",
    "    uniq_tokens += list(set(word_temps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5eoXeNgRpna"
   },
   "source": [
    "####  Third Step: \n",
    "\n",
    "Load the stop words dataset and Python's english stop words package to ensure removing the stop words. Put these two kinds of stop words list in the same list and drop the duplicate stop words. Finally removing the stop words in words bag that we have got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t7pdy4dMu0UM"
   },
   "outputs": [],
   "source": [
    "# Third Step: removing the stop words\n",
    "\n",
    "# Load the basic stopwords list\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "# Load the stopwords file\n",
    "directory_stopwords = \"/content/drive/Shareddrives/FIT5196-s1-2022/A1/Task2/stopwords_en.txt\"\n",
    "with open(directory_stopwords,'r') as infile:\n",
    "    stopwords = infile.read().splitlines()\n",
    "\n",
    "# concat two stopwords list\n",
    "for stopword in stopwords:\n",
    "  stopwords_list.append(stopword)\n",
    "stopwords_set = set(stopwords_list) \n",
    "\n",
    "# removing stopwords\n",
    "match_tokens = [] # no-stop words\n",
    "for each in uniq_tokens:\n",
    "    if each not in stopwords_set:\n",
    "        match_tokens.append(each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_GD6Mh_RsLK"
   },
   "source": [
    "####  Fourth Step: \n",
    "\n",
    "Now that we have the word bag under each date (no duplicate words), we are going to do the word count, using **FreqDist** function to calculate the word frequency. Then Removing the words that less than 10 days (i.e. 10 unique dates)) and more than ceil(Number_of_days / 2).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g863MehAOgHg"
   },
   "outputs": [],
   "source": [
    "# Fourth Step: removing the words that less than 10 days (i.e. 10 unique dates)) and more than ceil(Number_of_days / 2). \n",
    "\n",
    "# caculating the frequencies of unique words at each date\n",
    "freq_uniq_tokens = []\n",
    "frequency_tokens = FreqDist(match_tokens)\n",
    "\n",
    "# removing the words that less than 10 days and more than ceil(Number_of_days / 2)\n",
    "for token in frequency_tokens:\n",
    "    if frequency_tokens[token] >=10 and frequency_tokens[token] <= ceil(len(dict_word.keys())/2):\n",
    "        freq_uniq_tokens.append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7zPPg6IRtoW"
   },
   "source": [
    "####  Fifth Step: \n",
    "  \n",
    "  Using the **Porter Stemmer** to access the stem of word and removing the tokens that less than 3 by **for loop**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbclTRdVIGq_"
   },
   "outputs": [],
   "source": [
    "# Fifth step: Using the Porter Stemmer to access the stem of word\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stem_token = []\n",
    "for token in freq_uniq_tokens:\n",
    "    stem_token.append(stemmer.stem(token))\n",
    "\n",
    "# remove the tokens that less than 3    \n",
    "vocab_m3 = []\n",
    "for token in stem_token:\n",
    "    if len(token) > 2:\n",
    "      vocab_m3.append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eymfga98Rvcl"
   },
   "source": [
    "####  Sixth step: \n",
    "\n",
    "First of all, all the words are obtained through the for loop and word segmentation. After converting the words to lowercase, use the built-in method nbest under the **nltk package** to find out the 200 most meaningful phrases. These words are spliced and put into the list, and then **merged** with the previous words that have been cleared.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-CJCxPBPJ5pw"
   },
   "outputs": [],
   "source": [
    "# Sixth step: Get the 200 meaningful bigrams by pmi measure\n",
    "bigrams = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "token_list = [] # all tokens\n",
    "\n",
    "# split the sentence to words\n",
    "for date in dict_word:\n",
    "    words_string = \" \".join(dict_word[date])\n",
    "    token_list += re.findall(reg_tokens, words_string)\n",
    "\n",
    "# lower words\n",
    "tokens_list = []\n",
    "for token in token_list:\n",
    "    tokens_list.append(token.lower())\n",
    "\n",
    "# nltk package\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(tokens_list)\n",
    "meaning = finder.nbest(bigrams.pmi, 200)\n",
    "\n",
    "meaning_vocab = []\n",
    "for token in meaning:\n",
    "    word = token[0] + '_' + token[1]\n",
    "    meaning_vocab.append(word)\n",
    "# Get all the vocab \n",
    "all_vocab = meaning_vocab + vocab_m3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAbT5EfRkCxc"
   },
   "source": [
    "####  Seventh step: \n",
    "\n",
    "Sort these words in ascending alphabetical order by **sort funtion**, and then **zipper** the words that have been counted and the words we really need after cleaning. And finally write these data to the file by using the **open** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hcB-LoNKGWx"
   },
   "outputs": [],
   "source": [
    "# sort the word by first letter\n",
    "all_vocab.sort()\n",
    "\n",
    "num_list = []\n",
    "for i in range(len(all_vocab)):\n",
    "    num_list.append(i)\n",
    "\n",
    "# zip the two word list\n",
    "all_vocab_dict = dict(zip(all_vocab, num_list))\n",
    "\n",
    "# write the word to file\n",
    "res = ''\n",
    "for key, value in all_vocab_dict.items():\n",
    "    res += key + ':' + str(value) +'\\n'\n",
    "\n",
    "with open('/content/drive/MyDrive/2020 August - Master of Data Science/task2_sample_vocab.txt','w') as f:\n",
    "    f.write(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7WWj-LzRDtJ"
   },
   "source": [
    "##4.0 Creating the sparse matrix using countvectorizer.\n",
    "The main purpose of this part is to generate a matrix of words on different dates. Mainly through the method of **CountVectorizer** to achieve. Generate word bag, word frequency, and final word matrix, respectively.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kY2_UQTborYc"
   },
   "source": [
    "Initialize CountVectorizer:\n",
    "\n",
    " CountVectorizer function description: Create the word bag data structure\n",
    " \n",
    " Parameters:\n",
    "\n",
    "> Transfor the word to Lower case\n",
    "\n",
    "> Regular expression word segmentation\n",
    "\n",
    "> Drop stop words\n",
    "\n",
    "> According to unigram and bigram\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xejS2-teyjre"
   },
   "outputs": [],
   "source": [
    "vector = CountVectorizer(lowercase=True, token_pattern = \"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\",  stop_words=stopwords, ngram_range=(1, 2), analyzer='word')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFXCzrN3p6OS"
   },
   "source": [
    "Use a two-layer for loop to produce the result we want, generating a sparse matrix on demand. Loop through the dictionary with the date as the key generated before, iterate over each sentence in the value list, modify the format of the date, and then do the inner loop to convert the word into the number after the feature for concatenation, and finally write to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18475,
     "status": "ok",
     "timestamp": 1649381457670,
     "user": {
      "displayName": "Jingyi Chen",
      "userId": "01575192393336659630"
     },
     "user_tz": -600
    },
    "id": "ZJlmjcPkyTRf",
    "outputId": "a0e18aed-e588-4064-9f85-f816684ee2e7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "res = \"\"\n",
    "for key_date in dict_word: # dict_word structure: {date:[text,text]}\n",
    "    words_string = \" \".join(dict_word[key_date])\n",
    "\n",
    "    # Statistical results of word frequency: (0,2) 1 \n",
    "    # Used to process data, expressed in n-gram form\n",
    "    nums = vector.fit_transform([words_string]).toarray()[0] \n",
    "\n",
    "    name = vector.get_feature_names() # vocabulary list (a:0,b:1)\n",
    "\n",
    "    # date transform\n",
    "    trans_date = \"\"\n",
    "    key_date = re.split('\\s', key_date)\n",
    "\n",
    "    key_date[1] = key_date[1][:-1]\n",
    "    if len(key_date[0]) == 1:\n",
    "        key_date[0] = '0' + key_date[0]\n",
    "    if len(key_date[1]) == 1:\n",
    "        key_date[1] = '0' + key_date[1]\n",
    "        \n",
    "    trans_date = '/'.join(key_date) \n",
    "    \n",
    "    res += trans_date + ','\n",
    "\n",
    "    # Concatenate word numbers and word frequencies in the dictionary\n",
    "    daily_dic = {}\n",
    "    for i in range(len(name)):\n",
    "        daily_dic[name[i]] = nums[i]\n",
    "       \n",
    "    for i in range(len(name)):\n",
    "        each = name[i]\n",
    "        num = nums[i]\n",
    "        \n",
    "        if daily_dic[each] <= 0:\n",
    "            continue\n",
    "        words = each.split()\n",
    "        if len(words) == 2:\n",
    "            word = '_'.join(words)\n",
    "        else:\n",
    "            word = stemmer.stem(each)\n",
    "\n",
    "        ## check the word in the word bag\n",
    "        if word in all_vocab_dict:\n",
    "            res += str(all_vocab_dict[word]) + \":\" + str(num) + ','\n",
    "    res = res[:-1] + '\\n'\n",
    "\n",
    "## write the data to the file\n",
    "with open('/content/drive/MyDrive/2020 August - Master of Data Science/output_countVec.txt', 'w') as f:\n",
    "    f.write(res)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMPx9wKResMKZJpXUrXJ6HP",
   "collapsed_sections": [],
   "mount_file_id": "1_Ejiv8oNQyPgSUtJzFOs2hBIsS_1Qe4k",
   "name": "5196ASS1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
